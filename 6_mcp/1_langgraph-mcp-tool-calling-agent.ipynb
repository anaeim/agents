{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "- Define your LLM endpoint and system prompt\n",
    "- Configure databricks-managed MCP Servers for your agent\n",
    "- Convert an MCP tool definition into a LangChain-compatible tool.\n",
    "- Define agent logic using langgraph framework\n",
    "- Wrap the compiled agent and make it compatible with Mosaic AI Responses API\n",
    "- Evaluate your agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c2206f9-6eb0-497d-9571-ac9609bf68f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Mosaic AI Agent Framework: Author and deploy an MCP tool-calling LangGraph agent\n",
    "\n",
    "This notebook shows how to author a LangGraph agent that connects to MCP servers hosted on Databricks. You can choose between a Databricks-managed MCP server, a custom MCP server hosted as a Databricks app, or both simultaneously. To learn more about these options, see [MCP on Databricks](https://docs.databricks.com/aws/en/generative-ai/mcp/).\n",
    "\n",
    "\n",
    "This notebook uses the [`ResponsesAgent`](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ResponsesAgent) interface for compatibility with Mosaic AI features. In this notebook you learn to:\n",
    "\n",
    "- Author a LangGraph agent (wrapped with `ResponsesAgent`) that calls MCP tools\n",
    "- Manually test the agent\n",
    "- Evaluate the agent using Mosaic AI Agent Evaluation\n",
    "- Log and deploy the agent\n",
    "\n",
    "To learn more about authoring an agent using Mosaic AI Agent Framework, see Databricks documentation ([AWS](https://docs.databricks.com/aws/generative-ai/agent-framework/author-agent) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/create-chat-model)).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Address all `TODO`s in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0c20c06-17c4-4a2c-8316-292968c6f19d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mcp>=1.9 databricks-sdk[openai] databricks-agents>=1.0.0 databricks-mcp databricks-langchain uv langgraph==0.3.4\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f532673-48ca-4794-83c8-e9a13e3ac287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define the agent code\n",
    "\n",
    "Define the agent code in a single cell below. This lets you easily write the agent code to a local Python file, using the `%%writefile` magic command, for subsequent logging and deployment.\n",
    "\n",
    "The following cell creates a flexible, tool-using agent that integrates Databricks MCP servers with the Mosaic AI Agent Framework. Here‚Äôs what happens, at a high level:\n",
    "\n",
    "1. **MCP tool wrappers**  \n",
    "   Custom wrapper classes are defined so LangChain tools can interact with Databricks MCP servers. You can connect to Databricks-managed MCP servers, custom MCP servers hosted as a Databricks App, or both.\n",
    "\n",
    "2. **Tool discovery & registration**  \n",
    "   The agent automatically discovers available tools from the specified MCP server(s), turns their schemas into Python objects, and prepares them for the LLM.\n",
    "\n",
    "3. **Define the LangGraph agent logic**  \n",
    "   Define an agent workflow that does the following:\n",
    "   - The agent reads messages (conversation/history).\n",
    "   - If a tool (function) call is requested, it‚Äôs executed using the correct MCP tool.\n",
    "   - The agent can loop, performing multiple tool calls as needed, until a final answer is ready.\n",
    "\n",
    "4. **Wrap the LangGraph agent using the `ResponsesAgent` class**  \n",
    "   The agent is wrapped using `ResponsesAgent` so it's compatible with the Mosaic AI.\n",
    "   \n",
    "5. **MLflow autotracing**\n",
    "   Enable MLflow autologging to start automatic tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a467e77-fe23-4349-9a5d-8206ebe1e300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "import asyncio\n",
    "import mlflow\n",
    "import os\n",
    "import json\n",
    "from uuid import uuid4\n",
    "from pydantic import BaseModel, create_model\n",
    "from typing import Annotated, Any, Generator, List, Optional, Sequence, TypedDict, Union\n",
    "\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    VectorSearchRetrieverTool,\n",
    ")\n",
    "from databricks_mcp import DatabricksOAuthClientProvider, DatabricksMCPClient\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    "    convert_to_openai_messages,\n",
    ")\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client as connect\n",
    "\n",
    "from mlflow.entities import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "############################################\n",
    "## Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "# TODO: Replace with your model serving endpoint\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "# TODO: Update with your system prompt\n",
    "system_prompt = \"You are a helpful assistant that can run Python code.\"\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Configure MCP Servers for your agent\n",
    "## This section sets up server connections so your agent can retrieve data or take actions.\n",
    "###############################################################################\n",
    "\n",
    "# TODO: Choose your MCP server connection type.\n",
    "\n",
    "# ----- Simple: Managed MCP Server (no extra setup required) -----\n",
    "# Uses your Databricks Workspace settings and Personal Access Token (PAT) auth.\n",
    "workspace_client = WorkspaceClient()\n",
    "\n",
    "# Managed MCP Servers: Ready to use with default settings above\n",
    "host = workspace_client.config.host\n",
    "MANAGED_MCP_SERVER_URLS = [\n",
    "    f\"{host}/api/2.0/mcp/functions/system/ai\",\n",
    "]\n",
    "\n",
    "# ----- Advanced (optional): Custom MCP Server with OAuth -----\n",
    "# For Databricks Apps hosting custom MCP servers, OAuth with a service principal is required.\n",
    "# Uncomment and fill in your settings ONLY if connecting to a custom MCP server.\n",
    "#\n",
    "# import os\n",
    "# workspace_client = WorkspaceClient(\n",
    "#     host=\"<DATABRICKS_WORKSPACE_URL>\",\n",
    "#     client_id=os.getenv(\"DATABRICKS_CLIENT_ID\"),\n",
    "#     client_secret=os.getenv(\"DATABRICKS_CLIENT_SECRET\"),\n",
    "#     auth_type=\"oauth-m2m\",   # Enables machine-to-machine OAuth\n",
    "# )\n",
    "\n",
    "# Custom MCP Servers: Add URLs below if needed (requires custom setup and OAuth above)\n",
    "CUSTOM_MCP_SERVER_URLS = [\n",
    "    # Example: \"https://<custom-mcp-url>/mcp\"\n",
    "]\n",
    "\n",
    "#####################\n",
    "## MCP Tool Creation\n",
    "#####################\n",
    "\n",
    "# Define a custom LangChain tool that wraps functionality for calling MCP servers\n",
    "class MCPTool(BaseTool):\n",
    "    \"\"\"Custom LangChain tool that wraps MCP server functionality\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, description: str, args_schema: type, server_url: str, ws: WorkspaceClient, is_custom: bool = False):\n",
    "        # Initialize the tool\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            description=description,\n",
    "            args_schema=args_schema\n",
    "        )\n",
    "        # Store custom attributes: MCP server URL, Databricks workspace client, and whether the tool is for a custom server\n",
    "        object.__setattr__(self, 'server_url', server_url)\n",
    "        object.__setattr__(self, 'workspace_client', ws)\n",
    "        object.__setattr__(self, 'is_custom', is_custom)\n",
    "\n",
    "    def _run(self, **kwargs) -> str:\n",
    "        \"\"\"Execute the MCP tool\"\"\"\n",
    "        if self.is_custom:\n",
    "            # Use the async method for custom MCP servers (OAuth required)\n",
    "            return asyncio.run(self._run_custom_async(**kwargs))\n",
    "        else:\n",
    "            # Use managed MCP server via synchronous call\n",
    "            mcp_client = DatabricksMCPClient(server_url=self.server_url, workspace_client=self.workspace_client)\n",
    "            response = mcp_client.call_tool(self.name, kwargs)\n",
    "            return \"\".join([c.text for c in response.content])\n",
    "\n",
    "    async def _run_custom_async(self, **kwargs) -> str:\n",
    "        \"\"\"Execute custom MCP tool asynchronously\"\"\"        \n",
    "        async with connect(self.server_url, auth=DatabricksOAuthClientProvider(self.workspace_client)) as (\n",
    "            read_stream,\n",
    "            write_stream,\n",
    "            _,\n",
    "        ):\n",
    "            # Create an async session with the server and call the tool\n",
    "            async with ClientSession(read_stream, write_stream) as session:\n",
    "                await session.initialize()\n",
    "                response = await session.call_tool(self.name, kwargs)\n",
    "                return \"\".join([c.text for c in response.content])\n",
    "\n",
    "# Retrieve tool definitions from a custom MCP server (OAuth required)\n",
    "async def get_custom_mcp_tools(ws: WorkspaceClient, server_url: str):\n",
    "    \"\"\"Get tools from a custom MCP server using OAuth\"\"\"    \n",
    "    async with connect(server_url, auth=DatabricksOAuthClientProvider(ws)) as (\n",
    "        read_stream,\n",
    "        write_stream,\n",
    "        _,\n",
    "    ):\n",
    "        async with ClientSession(read_stream, write_stream) as session:\n",
    "            await session.initialize()\n",
    "            tools_response = await session.list_tools()\n",
    "            return tools_response.tools\n",
    "\n",
    "# Retrieve tool definitions from a managed MCP server\n",
    "def get_managed_mcp_tools(ws: WorkspaceClient, server_url: str):\n",
    "    \"\"\"Get tools from a managed MCP server\"\"\"\n",
    "    mcp_client = DatabricksMCPClient(server_url=server_url, workspace_client=ws)\n",
    "    return mcp_client.list_tools()\n",
    "\n",
    "# Convert an MCP tool definition into a LangChain-compatible tool.\n",
    "# why? you are going to create your agent through LangChain/LangGraph framework, and you need to run mcp server tools as LangChain tools, so you need to  make the tools compatible with LangChain: you need to convert mcp server tools into langchain tools.\n",
    "# how? you get the reqired info such name description, schema from mcp tool and define a method to run mcp tool (_run). Then, based on the info and _run method, you subclass langchain_core.tools.BaseModel (here MCPTool class) to create a langchain tool.\n",
    "def create_langchain_tool_from_mcp(mcp_tool, server_url: str, ws: WorkspaceClient, is_custom: bool = False):\n",
    "    \"\"\"Create a LangChain tool from an MCP tool definition\"\"\"\n",
    "    schema = mcp_tool.inputSchema.copy()\n",
    "    properties = schema.get(\"properties\", {})\n",
    "    required = schema.get(\"required\", [])\n",
    "\n",
    "    # Map JSON schema types to Python types for input validation\n",
    "    TYPE_MAPPING = {\n",
    "        \"integer\": int,\n",
    "        \"number\": float,\n",
    "        \"boolean\": bool\n",
    "    }\n",
    "    field_definitions = {}\n",
    "    for field_name, field_info in properties.items():\n",
    "        field_type_str = field_info.get(\"type\", \"string\")\n",
    "        field_type = TYPE_MAPPING.get(field_type_str, str)\n",
    "\n",
    "        if field_name in required:\n",
    "            field_definitions[field_name] = (field_type, ...)\n",
    "        else:\n",
    "            field_definitions[field_name] = (field_type, None)\n",
    "\n",
    "    # Dynamically create a Pydantic schema for the tool's input arguments\n",
    "    args_schema = create_model(\n",
    "        f\"{mcp_tool.name}Args\",\n",
    "        **field_definitions\n",
    "    )\n",
    "\n",
    "    # Return a configured MCPTool instance\n",
    "    return MCPTool(\n",
    "        name=mcp_tool.name,\n",
    "        description=mcp_tool.description or f\"Tool: {mcp_tool.name}\",\n",
    "        args_schema=args_schema,\n",
    "        server_url=server_url,\n",
    "        ws=ws,\n",
    "        is_custom=is_custom\n",
    "    )\n",
    "\n",
    "# Gather all tools from managed and custom MCP servers into a single list\n",
    "async def create_mcp_tools(ws: WorkspaceClient, \n",
    "                          managed_server_urls: List[str] = None, \n",
    "                          custom_server_urls: List[str] = None) -> List[MCPTool]:\n",
    "    \"\"\"Create LangChain tools from both managed and custom MCP servers\"\"\"\n",
    "    tools = []\n",
    "\n",
    "    if managed_server_urls:\n",
    "        # Load managed MCP tools\n",
    "        for server_url in managed_server_urls:\n",
    "            try:\n",
    "                mcp_tools = get_managed_mcp_tools(ws, server_url)\n",
    "                for mcp_tool in mcp_tools:\n",
    "                    tool = create_langchain_tool_from_mcp(mcp_tool, server_url, ws, is_custom=False)\n",
    "                    tools.append(tool)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading tools from managed server {server_url}: {e}\")\n",
    "\n",
    "    if custom_server_urls:\n",
    "        # Load custom MCP tools (async)\n",
    "        for server_url in custom_server_urls:\n",
    "            try:\n",
    "                mcp_tools = await get_custom_mcp_tools(ws, server_url)\n",
    "                for mcp_tool in mcp_tools:\n",
    "                    tool = create_langchain_tool_from_mcp(mcp_tool, server_url, ws, is_custom=True)\n",
    "                    tools.append(tool)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading tools from custom server {server_url}: {e}\")\n",
    "\n",
    "    return tools\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "# The state for the agent workflow, including the conversation and any custom data\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    custom_inputs: Optional[dict[str, Any]]\n",
    "    custom_outputs: Optional[dict[str, Any]]\n",
    "\n",
    "# Define the LangGraph agent that can call tools\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    "):\n",
    "    model = model.bind_tools(tools)  # Bind tools to the model\n",
    "\n",
    "    # Function to check if agent should continue or finish based on last message\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If function (tool) calls are present, continue; otherwise, end\n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    # Preprocess: optionally prepend a system prompt to the conversation history\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "\n",
    "    model_runnable = preprocessor | model  # Chain the preprocessor and the model\n",
    "\n",
    "    # The function to invoke the model within the workflow\n",
    "    def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)  # Create the agent's state machine\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))  # Agent node (LLM)\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))             # Tools node\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")  # Start at agent node\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",  # If the model requests a tool call, move to tools node\n",
    "            \"end\": END,           # Otherwise, end the workflow\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")  # After tools are called, return to agent node\n",
    "\n",
    "    # Compile and return the tool-calling agent workflow\n",
    "    return workflow.compile()\n",
    "\n",
    "# ResponsesAgent class to wrap the compiled agent and make it compatible with Mosaic AI Responses API\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    # Convert a Responses-style message to a ChatCompletion format\n",
    "    def _responses_to_cc(\n",
    "        self, message: dict[str, Any]\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Convert from a Responses API output item to ChatCompletion messages.\"\"\"\n",
    "        msg_type = message.get(\"type\")\n",
    "        if msg_type == \"function_call\":\n",
    "            # Format tool/function call messages\n",
    "            return [\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"tool call\",\n",
    "                    \"tool_calls\": [\n",
    "                        {\n",
    "                            \"id\": message[\"call_id\"],\n",
    "                            \"type\": \"function\",\n",
    "                            \"function\": {\n",
    "                                \"arguments\": message[\"arguments\"],\n",
    "                                \"name\": message[\"name\"],\n",
    "                            },\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "        elif msg_type == \"message\" and isinstance(message[\"content\"], list):\n",
    "            # Format regular content messages\n",
    "            return [\n",
    "                {\"role\": message[\"role\"], \"content\": content[\"text\"]}\n",
    "                for content in message[\"content\"]\n",
    "            ]\n",
    "        elif msg_type == \"reasoning\":\n",
    "            # Reasoning steps as assistant messages\n",
    "            return [{\"role\": \"assistant\", \"content\": json.dumps(message[\"summary\"])}]\n",
    "        elif msg_type == \"function_call_output\":\n",
    "            # Function/tool outputs\n",
    "            return [\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": message[\"output\"],\n",
    "                    \"tool_call_id\": message[\"call_id\"],\n",
    "                }\n",
    "            ]\n",
    "        # Pass through only the known, compatible fields\n",
    "        compatible_keys = [\"role\", \"content\", \"name\", \"tool_calls\", \"tool_call_id\"]\n",
    "        filtered = {k: v for k, v in message.items() if k in compatible_keys}\n",
    "        return [filtered] if filtered else []\n",
    "\n",
    "    # Convert a LangChain message to a Responses-format dictionary\n",
    "    def _langchain_to_responses(self, messages: list[BaseMessage]) -> list[dict[str, Any]]:\n",
    "        \"\"\"Convert from LangChain messages to Responses output item dictionaries\"\"\"\n",
    "        for message in messages:\n",
    "            message = message.model_dump()  # Convert the message model to dict\n",
    "            role = message[\"type\"]\n",
    "            if role == \"ai\":\n",
    "                if tool_calls := message.get(\"tool_calls\"):\n",
    "                    # Return function call items for all tool calls present\n",
    "                    return [\n",
    "                        self.create_function_call_item(\n",
    "                            id=message.get(\"id\") or str(uuid4()),\n",
    "                            call_id=tool_call[\"id\"],\n",
    "                            name=tool_call[\"name\"],\n",
    "                            arguments=json.dumps(tool_call[\"args\"]),\n",
    "                        )\n",
    "                        for tool_call in tool_calls\n",
    "                    ]\n",
    "                else:\n",
    "                    # Regular AI text message\n",
    "                    return [\n",
    "                        self.create_text_output_item(\n",
    "                            text=message[\"content\"],\n",
    "                            id=message.get(\"id\") or str(uuid4()),\n",
    "                        )\n",
    "                    ]\n",
    "            elif role == \"tool\":\n",
    "                # Output from tool/function execution\n",
    "                return [\n",
    "                    self.create_function_call_output_item(\n",
    "                        call_id=message[\"tool_call_id\"],\n",
    "                        output=message[\"content\"],\n",
    "                    )\n",
    "                ]\n",
    "            elif role == \"user\":\n",
    "                # User messages as-is\n",
    "                return [message]\n",
    "\n",
    "    # Make a prediction (single-step) for the agent\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\" or event.type == \"error\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    # Stream predictions for the agent, yielding output as it's generated\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        cc_msgs = []\n",
    "        for msg in request.input:\n",
    "            cc_msgs.extend(self._responses_to_cc(msg.model_dump()))\n",
    "\n",
    "        # Stream events from the agent graph\n",
    "        for event in self.agent.stream({\"messages\": cc_msgs}, stream_mode=[\"updates\", \"messages\"]):\n",
    "            if event[0] == \"updates\":\n",
    "                # Stream updated messages from the workflow nodes\n",
    "                for node_data in event[1].values():\n",
    "                    if \"messages\" in node_data:\n",
    "                        for item in self._langchain_to_responses(node_data[\"messages\"]):\n",
    "                            yield ResponsesAgentStreamEvent(type=\"response.output_item.done\", item=item)\n",
    "            elif event[0] == \"messages\":\n",
    "                # Stream generated text message chunks\n",
    "                try:\n",
    "                    chunk = event[1][0]\n",
    "                    if isinstance(chunk, AIMessageChunk) and (content := chunk.content):\n",
    "                        yield ResponsesAgentStreamEvent(\n",
    "                            **self.create_text_delta(delta=content, item_id=chunk.id),\n",
    "                        )\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "# Initialize the entire agent, including MCP tools and workflow\n",
    "def initialize_agent():\n",
    "    \"\"\"Initialize the agent with MCP tools\"\"\"\n",
    "    # Create MCP tools from the configured servers\n",
    "    mcp_tools = asyncio.run(create_mcp_tools(\n",
    "        ws=workspace_client,\n",
    "        managed_server_urls=MANAGED_MCP_SERVER_URLS,\n",
    "        custom_server_urls=CUSTOM_MCP_SERVER_URLS\n",
    "    ))\n",
    "\n",
    "    # Create the agent graph with an LLM, tool set, and system prompt (if given)\n",
    "    agent = create_tool_calling_agent(llm, mcp_tools, system_prompt)\n",
    "    return LangGraphResponsesAgent(agent)\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = initialize_agent()\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Annotated means in Python\n",
    "\n",
    "`typing.Annotated` is a way to attach metadata to a type hint.\n",
    "\n",
    "**General form:**\n",
    "\n",
    "``` python\n",
    "from typing import Annotated\n",
    "\n",
    "x: Annotated[int, \"this is extra info\"] = 5\n",
    "```\n",
    "\n",
    "-   The first argument is the actual type (`int` here).\n",
    "-   The extra arguments are metadata --- which tools/libraries can use\n",
    "    if they want.\n",
    "-   At runtime, `x` is still just an `int`. The metadata doesn't change\n",
    "    the type itself.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### How LangGraph uses it\n",
    "\n",
    "In your `AgentState`:\n",
    "\n",
    "``` python\n",
    "messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "```\n",
    "\n",
    "-   The **real type**: a sequence of LangChain `BaseMessage` objects\n",
    "    (your conversation history).\n",
    "-   The **metadata**: the function `add_messages` (imported from\n",
    "    `langgraph.graph.message`).\n",
    "\n",
    "LangGraph uses this metadata during **state updates**:\n",
    "\n",
    "-   Normally, when a state machine updates a field, it just overwrites\n",
    "    the old value.\n",
    "-   But if a field is `Annotated[..., add_messages]`, LangGraph knows:\\\n",
    "    üëâ instead of overwriting, **merge the old and new values using\n",
    "    `add_messages`** (which appends new messages onto the conversation\n",
    "    history).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnableLambda in LangChain\n",
    "\n",
    "In your code, `RunnableLambda` shows up in spots like this:\n",
    "\n",
    "```python\n",
    "if system_prompt:\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    )\n",
    "else:\n",
    "    preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "\n",
    "model_runnable = preprocessor | model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## What `RunnableLambda` is\n",
    "\n",
    "- It comes from **LangChain Core**:\n",
    "  ```python\n",
    "  from langchain_core.runnables import RunnableLambda\n",
    "  ```\n",
    "\n",
    "- It is a wrapper that lets you take **any Python function (or lambda)** and turn it into a **`Runnable`**.\n",
    "\n",
    "- A **`Runnable`** in LangChain is an abstraction for \"things that can be invoked\" (LLMs, retrievers, tools, chains). They all share methods like `.invoke()`, `.ainvoke()`, `.batch()`, `.stream()`, and can be composed with operators like `|`.\n",
    "\n",
    "- By wrapping a function in `RunnableLambda`, you can plug it into a pipeline/graph alongside LLMs and tools.\n",
    "\n",
    "---\n",
    "\n",
    "#### Minimal Example\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# A plain Python function\n",
    "def double(x: int) -> int:\n",
    "    return x * 2\n",
    "\n",
    "# Wrap it\n",
    "double_runnable = RunnableLambda(double)\n",
    "\n",
    "print(double_runnable.invoke(5))        # 10\n",
    "print(double_runnable.batch([1, 2, 3])) # [2, 4, 6]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### In your agent code\n",
    "\n",
    "```python\n",
    "preprocessor = RunnableLambda(\n",
    "    lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    ")\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "- The `RunnableLambda` wraps a small lambda function that **injects a system prompt** at the start of the message list.\n",
    "- Because it‚Äôs a `Runnable`, you can compose it with the model like this:\n",
    "\n",
    "```python\n",
    "model_runnable = preprocessor | model\n",
    "```\n",
    "\n",
    "That means:\n",
    "1. Take the state ‚Üí run it through the preprocessor (`RunnableLambda`) ‚Üí outputs a list of messages.\n",
    "2. Pipe that into the model (`ChatDatabricks`), which is also a `Runnable`.\n",
    "\n",
    "So the model always sees the system prompt plus the conversation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why `asyncio.run` is Needed\n",
    "\n",
    "In your code, `asyncio.run` is used here:\n",
    "\n",
    "```python\n",
    "return asyncio.run(self._run_custom_async(**kwargs))\n",
    "```\n",
    "\n",
    "Let's break it down.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1Ô∏è‚É£ The Problem\n",
    "\n",
    "Python has two kinds of functions:\n",
    "\n",
    "1. **Synchronous (`def`) functions**\n",
    "   - Run top-to-bottom and return a result immediately.\n",
    "\n",
    "2. **Asynchronous (`async def`) functions**\n",
    "   - Return a *coroutine object* when called.\n",
    "   - You **cannot call them like normal functions** ‚Äî you need to `await` them.\n",
    "\n",
    "```python\n",
    "async def fetch_data():\n",
    "    await asyncio.sleep(1)\n",
    "    return \"done\"\n",
    "\n",
    "result = fetch_data()  # ‚ùå result is a coroutine, not the final value\n",
    "```\n",
    "\n",
    "- To get the actual result, you need an **event loop**.\n",
    "- `await fetch_data()` only works inside another `async def`.\n",
    "\n",
    "\n",
    "## solution:\n",
    "you run `asyncio.run(coro)` at high level and you can schedule the coroutine outside `async def`\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ What `asyncio.run` Does\n",
    "\n",
    "`asyncio.run(coro)`:\n",
    "\n",
    "1. Creates a fresh **event loop**.\n",
    "2. Runs the coroutine `coro` until it finishes.\n",
    "3. Returns the final result.\n",
    "4. Closes the loop automatically.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def greet(name):\n",
    "    await asyncio.sleep(1)\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "result = asyncio.run(greet(\"Alice\"))\n",
    "print(result)  # Hello, Alice!\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fa439a1-b043-4b9c-b722-5022dc536675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent\n",
    "\n",
    "Interact with the agent to test its output and tool-calling abilities. Since this notebook called `mlflow.langchain.autolog()`, you can view the trace for each step the agent takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f821c8bb-feb3-4e15-a531-cf86c73419af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92b052d2-bb0f-4abe-977c-0e136a3e7dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TODO: ONLY UNCOMMENT AND EDIT THIS SECTION IF YOU ARE USING OAUTH/SERVICE PRINCIPAL FOR CUSTOM MCP SERVERS.\n",
    "#       For managed MCP (the default), LEAVE THIS SECTION COMMENTED OUT.\n",
    "# ==============================================================================\n",
    "\n",
    "# import os\n",
    "\n",
    "# # Set your Databricks client ID and client secret for service principal authentication.\n",
    "# DATABRICKS_CLIENT_ID = \"<YOUR_CLIENT_ID>\"\n",
    "# client_secret_scope_name = \"<YOUR_SECRET_SCOPE>\"\n",
    "# client_secret_key_name = \"<YOUR_SECRET_KEY_NAME>\"\n",
    "\n",
    "# # Load your service principal credentials into environment variables\n",
    "# os.environ[\"DATABRICKS_CLIENT_ID\"] = DATABRICKS_CLIENT_ID\n",
    "# os.environ[\"DATABRICKS_CLIENT_SECRET\"] = dbutils.secrets.get(scope=client_secret_scope_name, key=client_secret_key_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9107d392-62e6-41cc-8e87-afb3b72abefe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "AGENT.predict({\"input\": [{\"role\": \"user\", \"content\": \"What is 7*6 in Python?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91a4e237-9b98-4b28-9aad-47575eb27d7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for chunk in AGENT.predict_stream(\n",
    "    {\"input\": [{\"role\": \"user\", \"content\": \"What is 7*6 in Python?\"}]}\n",
    "):\n",
    "    print(chunk, \"-----------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5212ddb-9de5-4a7e-ad56-bfc63e4921a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log the agent as an MLflow model\n",
    "\n",
    "Log the agent as code from the `agent.py` file. See [Deploy an agent that connects to Databricks MCP servers](https://docs.databricks.com/aws/en/generative-ai/mcp/managed-mcp#deploy-your-agent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6b21b44-8af6-4e85-9fa7-290d5425c139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from agent import LLM_ENDPOINT_NAME\n",
    "from mlflow.models.resources import DatabricksServingEndpoint, DatabricksFunction\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME), \n",
    "    DatabricksFunction(function_name=\"system.ai.python_exec\")\n",
    "]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            \"databricks-mcp\",\n",
    "            f\"mlflow=={get_distribution('mlflow').version}\",\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "            f\"mcp=={get_distribution('mcp').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad70dfa7-ad7c-4532-abaf-59a02d97755a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate the agent with [Agent Evaluation](https://docs.databricks.com/mlflow3/genai/eval-monitor)\n",
    "\n",
    "You can edit the requests or expected responses in your evaluation dataset and run evaluation as you iterate your agent, leveraging mlflow to track the computed quality metrics.\n",
    "\n",
    "Evaluate your agent with one of our [predefined LLM scorers](https://docs.databricks.com/mlflow3/genai/eval-monitor/predefined-judge-scorers), or try adding [custom metrics](https://docs.databricks.com/mlflow3/genai/eval-monitor/custom-scorers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09f9d352-e371-47a7-a5fd-bf3f6631e382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import RelevanceToQuery, Safety, RetrievalRelevance, RetrievalGroundedness\n",
    "\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Calculate the 15th Fibonacci number\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"expected_response\": \"The 15th Fibonacci number is 610.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=lambda input: AGENT.predict({\"input\": input}),\n",
    "    scorers=[RelevanceToQuery(), Safety()], # add more scorers here if they're applicable\n",
    ")\n",
    "\n",
    "# Review the evaluation results in the MLfLow UI (see console output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d97f7b2-e9fe-4bb7-b2e7-a8b84d0cffcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pre-deployment agent validation\n",
    "Before registering and deploying the agent, perform pre-deployment checks using the [mlflow.models.predict()](https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.predict) API. See Databricks documentation ([AWS](https://docs.databricks.com/en/machine-learning/model-serving/model-serving-debug.html#validate-inputs) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/model-serving-debug#before-model-deployment-validation-checks))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45dcfbdc-6890-4a40-8f96-6931a2d12a21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"What is 7*6 in Python?\"}]},\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f3f840-6c44-494a-bd78-806bb4b4faac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register the model to Unity Catalog\n",
    "\n",
    "Before you deploy the agent, you must register the agent to Unity Catalog.\n",
    "\n",
    "- **TODO** Update the `catalog`, `schema`, and `model_name` below to register the MLflow model to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1281a791-a49d-4f60-a1ab-6397f4d89005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = \"\"\n",
    "schema = \"\"\n",
    "model_name = \"langgraph-mcp-responses-agent\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15281971-d4a6-4dd1-b40c-f54db3b13f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a52668a-9e10-4d96-814d-f73f5324315a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "agents.deploy(\n",
    "    UC_MODEL_NAME, \n",
    "    uc_registered_model_info.version,\n",
    "    # ==============================================================================\n",
    "    # TODO: ONLY UNCOMMENT AND CONFIGURE THE ENVIRONMENT_VARS SECTION BELOW\n",
    "    #       IF YOU ARE USING OAUTH/SERVICE PRINCIPAL FOR CUSTOM MCP SERVERS.\n",
    "    #       For managed MCP (the default), LEAVE THIS SECTION COMMENTED OUT.\n",
    "    # ==============================================================================\n",
    "    # environment_vars={\n",
    "    #     \"DATABRICKS_CLIENT_ID\": DATABRICKS_CLIENT_ID,\n",
    "    #     \"DATABRICKS_CLIENT_SECRET\": f\"{{{{secrets/{client_secret_scope_name}/{client_secret_key_name}}}}}\"\n",
    "    # },\n",
    "    tags = {\"endpointSource\": \"docs\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7feb3be-c03c-4e79-9c40-028ccf5317a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "After your agent is deployed, you can chat with it in AI playground to perform additional checks, share it with SMEs in your organization for feedback, or embed it in a production application. See Databricks documentation ([AWS](https://docs.databricks.com/en/generative-ai/deploy-agent.html) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/deploy-agent))."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "langgraph-mcp-tool-calling-agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
