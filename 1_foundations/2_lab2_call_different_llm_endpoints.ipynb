{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key not set (and this is optional)\n",
      "DeepSeek API Key not set (and this is optional)\n",
      "Groq API Key not set (and this is optional)\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you were tasked with designing an educational system that prioritizes both critical thinking and emotional intelligence, what foundational principles would you incorporate, and how would you measure the effectiveness of this system over time?\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Designing an educational system that prioritizes critical thinking and emotional intelligence involves establishing a set of foundational principles that foster these skills and creating a robust framework for assessing their development over time. Here are some key principles and measurement approaches:\n",
       "\n",
       "### Foundational Principles\n",
       "\n",
       "1. **Holistic Curriculum Design**:\n",
       "   - Integrate critical thinking and emotional intelligence into all subjects rather than treating them as separate disciplines. \n",
       "   - Use interdisciplinary approaches that allow students to apply these skills across various contexts.\n",
       "\n",
       "2. **Inquiry-Based Learning**:\n",
       "   - Encourage students to ask questions, conduct research, and explore real-world problems.\n",
       "   - Foster a classroom environment where curiosity and exploration are prioritized, allowing students to engage in discussions and debates.\n",
       "\n",
       "3. **Collaborative Learning Environments**:\n",
       "   - Promote teamwork through group projects and peer-learning opportunities, which help students practice empathy, communication, and social skills.\n",
       "   - Use peer feedback as a tool to enhance both critical thinking and emotional intelligence.\n",
       "\n",
       "4. **Reflective Practices**:\n",
       "   - Implement regular self-assessment and reflection exercises that encourage students to evaluate their thinking processes and emotional responses. \n",
       "   - Utilize journals, portfolios, and personal reflections as tools for students to articulate their learning experiences.\n",
       "\n",
       "5. **Emotional and Social Learning (SEL)**:\n",
       "   - Introduce a comprehensive SEL program that includes recognizing one’s emotions, understanding others' emotions, managing interpersonal relationships, and making responsible decisions.\n",
       "   - Encourage practices like mindfulness and emotional regulation exercises within the classroom.\n",
       "\n",
       "6. **Mentorship and Role Models**:\n",
       "   - Pair students with mentors who exemplify strong emotional intelligence and critical thinking. This real-world model can inspire students to develop similar traits.\n",
       "   - Provide professional development for educators to enhance their own emotional intelligence and facilitate effective teaching of these concepts.\n",
       "\n",
       "7. **Diverse Learning Experiences**:\n",
       "   - Incorporate diverse perspectives in the curriculum to challenge assumptions and build critical analysis skills.\n",
       "   - Address global issues to increase students’ awareness of social justice, ethics, and multicultural understanding.\n",
       "\n",
       "### Measuring Effectiveness\n",
       "\n",
       "1. **Qualitative Assessments**:\n",
       "   - Use reflective essays, project-based assessments, and portfolios that require students to demonstrate critical thinking and emotional intelligence in their work.\n",
       "   - Conduct interviews or focus groups with students and teachers to gather insights on their experiences and perceptions of growth in these areas.\n",
       "\n",
       "2. **Surveys and Self-Assessments**:\n",
       "   - Implement standardized surveys measuring emotional intelligence and critical thinking before and after specific interventions. Tools like the Emotional Quotient Inventory (EQ-i) or the California Critical Thinking Skills Test can be used.\n",
       "   - Encourage self-reporting measures where students assess their emotional and critical thinking skills over time.\n",
       "\n",
       "3. **Performance Metrics**:\n",
       "   - Analyze outcomes on assessments that involve problem-solving, conflict resolution, and critical analysis, looking for trends in performance across diverse groups of students.\n",
       "   - Measure engagement in collaborative tasks and the ability to navigate social situations effectively.\n",
       "\n",
       "4. **Longitudinal Studies**:\n",
       "   - Track students over multiple years to see how skills in critical thinking and emotional intelligence develop and influence academic and social outcomes.\n",
       "   - Collaborate with higher education institutions and employers to assess the impact of skills learned in this system on post-graduation success.\n",
       "\n",
       "5. **Feedback Mechanisms**:\n",
       "   - Establish a system for ongoing feedback from students, parents, and educators to gauge the perceived effectiveness of the curriculum and approach over time.\n",
       "   - Use this feedback to iteratively refine the educational program, ensuring it remains responsive to the needs of the students.\n",
       "\n",
       "By embedding these foundational principles into an educational framework and employing varied measurement strategies, we can create a learning environment that not only values critical thinking and emotional intelligence but also systematically tracks and enhances these capabilities over time."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The API we know well\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Designing an Education System for Critical Thinking and Emotional Intelligence\n",
       "\n",
       "## Foundational Principles\n",
       "\n",
       "1. **Integrated Curriculum Design**: Knowledge domains shouldn't exist in isolation. I'd create frameworks where academic content naturally incorporates both analytical reasoning and emotional understanding.\n",
       "\n",
       "2. **Developmental Appropriateness**: Skills would be scaffolded according to cognitive and emotional development stages, with increasing complexity as students mature.\n",
       "\n",
       "3. **Teacher as Facilitator**: Educators would shift from knowledge-transmitters to learning coaches who model both intellectual rigor and emotional awareness.\n",
       "\n",
       "4. **Psychological Safety**: Learning environments must feel safe for intellectual risk-taking and emotional vulnerability.\n",
       "\n",
       "5. **Real-world Application**: Learning would connect to authentic problems requiring both critical analysis and interpersonal navigation.\n",
       "\n",
       "6. **Metacognitive Practice**: Students would regularly reflect on both their thinking processes and emotional responses.\n",
       "\n",
       "## Measurement Framework\n",
       "\n",
       "**Short-term indicators:**\n",
       "- Performance-based assessments measuring application of critical thinking to complex problems\n",
       "- Self-awareness metrics through reflection portfolios\n",
       "- Observational data on conflict resolution and collaborative problem-solving\n",
       "- Peer and self-evaluation of emotional regulation during challenging tasks\n",
       "\n",
       "**Medium-term indicators:**\n",
       "- Longitudinal tracking of decision-making processes\n",
       "- Growth in perspective-taking ability\n",
       "- Transfer of skills across different contexts\n",
       "- Decreased behavioral incidents/increased positive social interactions\n",
       "\n",
       "**Long-term indicators:**\n",
       "- Post-graduation outcomes including career satisfaction and advancement\n",
       "- Community engagement and civic participation\n",
       "- Relationship stability and quality\n",
       "- Adaptability to changing circumstances and novel challenges\n",
       "\n",
       "What aspects of this framework would you like me to elaborate on further?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_0141EUJYF5pHeTSemCZWmRtN', content=[TextBlock(citations=None, text='How might we reconcile the inherent tension between global coordination needed for existential risks and the value of cultural/political sovereignty, particularly when different societies have fundamentally different conceptions of progress or human flourishing?', type='text')], model='claude-3-7-sonnet-20250219', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=41, output_tokens=46, server_tool_use=None, service_tier='standard', cache_creation={'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}))\n"
     ]
    }
   ],
   "source": [
    "# take a look into LLm response from Anthropic LLM \n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "\n",
    "print(response)\n",
    "\n",
    "# answer = response.content[0].text\n",
    "# display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Message(id='msg_0141EUJYF5pHeTSemCZWmRtN', content=[TextBlock(citations=None, text='How might we reconcile the inherent tension between global coordination needed for existential risks and the value of cultural/political sovereignty, particularly when different societies have fundamentally different conceptions of progress or human flourishing?', type='text')], model='claude-3-7-sonnet-20250219', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=41, output_tokens=46, server_tool_use=None, service_tier='standard', cache_creation={'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**. why we use `OpenAI` to call gemini or deepseek?<br>\n",
    "A. You might be wondering: How come we’re using OpenAI’s library to call Gemini?\n",
    "\n",
    "The key is that the OpenAI Python client isn’t an LLM itself—it’s just a lightweight wrapper around HTTP calls. It takes your request (structured as JSON with things like lists of dicts) and sends it to a model endpoint.\n",
    "\n",
    "When OpenAI first released their API, it became hugely popular, and many other providers decided to adopt the same request/response format. That means their endpoints look and behave like OpenAI’s.\n",
    "\n",
    "Because of this, most providers (with the notable exception of Anthropic, at least today) expose OpenAI-compatible endpoints. To make it even easier, OpenAI’s Python client lets you specify a base URL. So instead of calling OpenAI’s servers, you can redirect it to Google’s endpoints—or anyone else’s—as long as they follow the same API spec.\n",
    "\n",
    "Google, for example, provides Gemini endpoints in their own format, but they also host special OpenAI-compatible endpoints. These are deliberately structured so that you can use the exact same OpenAI client library to call Gemini, simply by pointing the base URL at Google’s endpoint.\n",
    "\n",
    "In short: the OpenAI client is just a request wrapper, and many providers now mimic OpenAI’s API spec, so you can use the same code to call different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deepseek model\n",
    "- deepseek-chat:\n",
    "- deepseek-reasoning: R1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**. what is groq?<br>\n",
    "**A**. As I mentioned, we’re now going to look at Groq—that’s Groq with a Q, not Grok with a K.\n",
    "\n",
    "Groq is a provider focused on fast inference using their own specialized hardware. They’ve built custom chips that are highly optimized for running LLM workloads at speed.\n",
    "\n",
    "Like many others, Groq also supports the OpenAI client library, so you can call their models using the same familiar interface. And, of course, they also provide their own client library if you want to use that directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#ff7800;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**. what is `{{` here?<br>\n",
    "A. one nice trick to know is that if you actually want to have a curly brace within your string, then you can do that by having two curly braces in an F string. So that's why there are two curly braces here, because we actually want one curly brace to appear in the string itself. We're not. We don't want we don't want this to be interpreted as code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
